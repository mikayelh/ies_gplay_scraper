{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cufflinks as cf\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.offline as offline\n",
    "import plotly.tools as tls\n",
    "\n",
    "import nltk as nltk\n",
    "from nltk import bigrams \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import scattertext as st\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import textblob\n",
    "from textblob import Word\n",
    "from textblob import TextBlob\n",
    "\n",
    "#from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgword(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  C:\\Users\\Mikayel\\Desktop\\Python\\ies_gplay_scraper\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load all the data we have scraped. We have dataframes of 3 columns. We have the amount of the **support** the comment received (net of the likes and dislikes to a comment), the actual contents of the **comment** and the final **rating** to the app given by the users (on a scale from 1 to 5; 5 being the highest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-21f350436190>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# batch loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'data/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'reviews'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# additing the file definition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not list"
     ]
    }
   ],
   "source": [
    "files = [i for i in os.listdir('data') if 'reviews' in i]\n",
    "\n",
    "# preparation of renaming files\n",
    "nicks = ['fb', 'snap', 'whatsapp', 'musicaly']\n",
    "renamer = [[val, nicks[i]] for i,val in enumerate(files)]\n",
    "\n",
    "# batch loading\n",
    "load = [pd.read_csv(i) for i in 'data/' + os.listdir('data') if 'reviews' in i]\n",
    "\n",
    "# additing the file definition\n",
    "load = [val.assign(app = renamer[i][1]) for i, val in enumerate(load)]\n",
    "\n",
    "# concat into a DataFrame\n",
    "dt = pd.concat(load)\n",
    "dt.head()\n",
    "\n",
    "\n",
    "fb = dt.query('app == \"fb\"')\n",
    "snap = dt.query('app == \"snap\"')\n",
    "musicaly = dt.query('app == \"musicaly\"')\n",
    "whatsapp = dt.query('app == \"whatsapp\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets at a glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Facebook, there are {} observations and {} features in this dataset.\".format(dt.query('app == \"fb\"').shape[0],dt.query('app == \"fb\"').shape[1]))\n",
    "print(\"For Snapchat, there are {} observations and {} features in this dataset.\".format(dt.query('app == \"snap\"').shape[0],dt.query('app == \"snap\"').shape[1]))\n",
    "print(\"For Musicaly, there are {} observations and {} features in this dataset.\".format(dt.query('app == \"musicaly\"').shape[0],dt.query('app == \"musicaly\"').shape[1]))\n",
    "print(\"For Whatsapp, there are {} observations and {} features in this dataset.\\n\".format(dt.query('app == \"whatsapp\"').shape[0],dt.query('app == \"whatsapp\"').shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    \"Total ratings\",\n",
    "    dt.describe(),\n",
    "    \"Facebook ratings\",\n",
    "    dt.query('app == \"fb\"').describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean rating for the Facebook sample is 2.58, below of total app average of 2.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.loc[fb['support'] == fb.support.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest support among Facebook reviewswas received by a complaint that had a 2-star rating, mentioning such problems like video or chat bubble crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = fb.review[8]\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a dummy variable, that equals unity if the review is positively rated (higher than 3)\n",
    "dt['Positively_Rated'] = np.where(dt['rating']>3, 1, 0)\n",
    "\n",
    "# add average word length as a variable\n",
    "dt['word_count'] = dt['review'].apply(lambda x: len(str(x).split(\" \")))\n",
    "dt['char_count'] = dt['review'].str.len() ## this also includes spaces\n",
    "dt['avgword'] = dt['review'].apply(lambda x: avgword(x))\n",
    "\n",
    "#stopwords\n",
    "stop = stopwords.words('english')\n",
    "dt['stopwords'] = dt['review'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "# number of numerics and uppercase words\n",
    "dt['numerics'] = dt['review'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "dt['upper'] = dt['review'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "dt[['review','numerics','upper']].head()\n",
    "\n",
    "#remove uppercase and punctuation\n",
    "dt['review'] = dt['review'].apply(lambda x: \" \".join(x.lower() for x in x.split())).str.replace('[^\\w\\s]','')\n",
    "\n",
    "#remove stopwords\n",
    "dt['review'] = dt['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dt['polarity'] = dt['review'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2,2 , figsize=(7, 7), sharex=True)\n",
    "\n",
    "_ = sns.distplot(fb[\"rating\"] , color=\"skyblue\", ax=axes[0, 0], axlabel =\"Facebook rating\")\n",
    "_ = sns.distplot( snap[\"rating\"] , color=\"#34495e\", ax=axes[0, 1],axlabel =\"Snapchat rating\")\n",
    "_ = sns.distplot( musicaly[\"rating\"] , color=\"gold\", ax=axes[1, 0],axlabel =\"Musicaly rating\")\n",
    "_ = sns.distplot( whatsapp[\"rating\"] , color=\"teal\", ax=axes[1, 1],axlabel =\"Whatsapp rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst Snapchat and Facebook have more negative 1-star reviews than positive (Facebook has the highest amount of 1-star reviews - 5752), Musicaly/TikTok seems to be very favourably rated by its users, with the highest amount of favourable 5 stars, 5725."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.groupby(['app', 'rating']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the classification model, we get rid of the neutral reviews, which are generally regard as 3 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a dummy variable, that equals unity if the review is positively rated (higher than 3)\n",
    "fb['Positively_Rated'] = np.where(fb['rating']>3, 1, 0)\n",
    "fb.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb[['Positively_Rated']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 14080 reviews in our sample, only 37.3% are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index = fb['Positively_Rated'], columns=\"Total count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['word_count'] = fb['review'].apply(lambda x: len(str(x).split(\" \")))\n",
    "fb['char_count'] = fb['review'].str.len() ## this also includes spaces\n",
    "\n",
    "fb.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average word length as a variable\n",
    "fb['avgword'] = fb['review'].apply(lambda x: avgword(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the amount of stopwords, numbers and special characters in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "fb['stopwords'] = fb['review'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "fb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of numerics and uppercase words\n",
    "fb['numerics'] = fb['review'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "fb['upper'] = fb['review'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "fb[['review','numerics','upper']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the data for text mining and further analysis by pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove uppercase\n",
    "fb['review'] = fb['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "#remove punctuation\n",
    "fb['review'] = fb['review'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#remove stopwords\n",
    "fb['review'] = fb['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "fb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_count = dt['word_count'].mean()\n",
    "avg_word_count_snap=dt[dt['app']==\"snap\"][\"word_count\"].mean()\n",
    "avg_word_count_fb=dt[dt['app']==\"fb\"][\"word_count\"].mean()\n",
    "avg_word_count_musicaly=dt[dt['app']==\"musicaly\"][\"word_count\"].mean()\n",
    "avg_word_count_whatsapp=dt[dt['app']==\"whatsapp\"][\"word_count\"].mean()\n",
    "\n",
    "pd.DataFrame([avg_word_count_fb, avg_word_count_snap, avg_word_count_musicaly, avg_word_count_whatsapp,avg_word_count], index = ['Fb', 'Snap','Musicaly','Whatsapp','Total'], columns = ['average']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, Snapchat's reviewers seem to be the wordiest, with the average word count of the review being the largest - 45.18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.loc[:,['polarity', 'word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_word_count =dt['word_count'].mean()\n",
    "\n",
    "f, axes = plt.subplots(2,2 , figsize=(10, 10), sharex=True)\n",
    "_ = sns.kdeplot(dt[dt['app']==\"fb\"][\"word_count\"] , color=\"skyblue\", ax=axes[0, 0],shade=True)\n",
    "_ = sns.kdeplot(dt[dt['app']==\"snap\"][\"word_count\"] , color=\"#34495e\", ax=axes[0, 1],shade=True)                #,ax =\"Snapchat rating\")\n",
    "_ = sns.kdeplot(dt[dt['app']==\"musicaly\"][\"word_count\"] , color=\"gold\", ax=axes[1, 0],shade=True)               #,ax =\"Musicaly rating\")\n",
    "_ = sns.kdeplot(dt[dt['app']==\"whatsapp\"][\"word_count\"] , color=\"teal\", ax=axes[1, 1],shade=True)               #,ax =\"Whatsapp rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would also be interesting to see, whether or not the little bump in the amount of word count (between 100 and 150 words) is connected to a higher negative sentiment (one would expect disappointed users to have higher commitment to writing the review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = dt.iloc[np.random.randint(1, 46760, 1000)]\n",
    "\n",
    "plt.scatter(samp.word_count, samp.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this seems not to be the case - since the dispersion of the sentiment is seemingly random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.heatmap(dt[['rating','support','Positively_Rated','word_count','avgword','stopwords','polarity']].corr().round(3), annot=True, linewidth=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also lemmatize the reviews\n",
    "fb['review'] = fb['review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "fb[['review']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(fb['review']).split()).value_counts()[:15]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use  TextBlob to calculate sentiment polarity (ranging betweeen -1 and 1; where 1 means positive sentiment and -1 means a negative sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['polarity'] = fb['review'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "fb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Sentence tokenizer splits paragraph text into sentences, whilst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all rows into one text\n",
    "text=fb[\"review\"].str.cat(sep=' ')\n",
    "tokenized_text=sent_tokenize(text)\n",
    "#print(tokenized_text)\n",
    "\n",
    "tokenized_word=word_tokenize(text)\n",
    "#print(tokenized_word)\n",
    "\n",
    "fdist = FreqDist(tokenized_word)\n",
    "#print(fdist)\n",
    "#fdist.most_common(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 most frequent words\n",
    "\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all rows into one text\n",
    "text=dt[\"review\"].str.cat(sep=' ')\n",
    "tokenized_text=sent_tokenize(text)\n",
    "\n",
    "fdistall = FreqDist(tokenized_word)\n",
    "fdistall.plot(30,cumulative=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['app', 'messenger', 'update','message','cant']\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic,'more :', ' , '.join([ word.lower() for word, count in fdist.most_common(5)]))\n",
    "    print(topic,'less :', ' , '.join([ word.lower() for word, count in fdist.most_common()[-5:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['parsed'] = dt.review.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dt.query('app == \"fb\"').review.str.split(' ')\n",
    "fb_top15 = words.apply(pd.Series).melt().dropna().groupby('value').size().sort_values(ascending = False).iloc[:15]\n",
    "words = dt.query('app == \"whatsapp\"').review.str.split(' ')\n",
    "what_top15 = words.apply(pd.Series).melt().dropna().groupby('value').size().sort_values(ascending = False).iloc[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame((fb_top15, what_top15), index = ['fb','whatsapp']).T.dropna()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
